{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VK1843/Varunkumar_INFO5731_Fall2025/blob/main/Chennuri_Varun_Assignment_2_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Monday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (25 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "\n",
        "(3) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(4) Collect all the information of the 904 narrators in the Densho Digital Repository.\n",
        "\n",
        "(5)**Collect a total of 10000 reviews** of the top 100 most popular software from G2 and Capterra.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jDyTKYs-yGit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d3f91d4-1ea2-43f4-90d2-2ca12ff35b21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install pandas requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# --- Configuration ---\n",
        "API_BASE_URL = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
        "QUERY = \"machine learning\"\n",
        "TARGET_PAPERS = 1000\n",
        "PAGE_LIMIT = 100 # Max number of results per API call (Semantic Scholar limit)\n",
        "CSV_FILENAME = \"semantic_scholar_abstracts.csv\"\n",
        "SLEEP_TIME_SECONDS = 5  # Recommended delay to respect API rate limits\n",
        "\n",
        "def fetch_data_from_api(query: str, offset: int) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Fetches a single page of data from the Semantic Scholar API.\n",
        "    Handles basic error checking and rate limiting.\n",
        "    \"\"\"\n",
        "    params = {\n",
        "        'query': query,\n",
        "        'offset': offset,\n",
        "        'limit': PAGE_LIMIT,\n",
        "        # Specify the fields we want to retrieve\n",
        "        'fields': 'paperId,title,abstract,authors,year'\n",
        "    }\n",
        "\n",
        "    print(f\"-> Fetching papers starting at offset {offset}...\")\n",
        "\n",
        "    try:\n",
        "        response = requests.get(API_BASE_URL, params=params)\n",
        "        response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n",
        "        return response.json()\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        if response.status_code == 429:\n",
        "            print(f\"Rate limit hit (429). Sleeping for {SLEEP_TIME_SECONDS * 2} seconds.\")\n",
        "            time.sleep(SLEEP_TIME_SECONDS * 2)\n",
        "            # Try again after waiting\n",
        "            return fetch_data_from_api(query, offset)\n",
        "        else:\n",
        "            print(f\"HTTP Error occurred: {e}\")\n",
        "            return None\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"An error occurred during API request: {e}\")\n",
        "        return None\n",
        "\n",
        "def collect_and_save_abstracts():\n",
        "    \"\"\"\n",
        "    Main function to collect abstracts, handle pagination, and save to CSV.\n",
        "    \"\"\"\n",
        "    all_abstracts: List[Dict[str, Any]] = []\n",
        "    current_offset = 0\n",
        "    total_collected = 0\n",
        "\n",
        "    # Loop until the target number of papers is met or no more results are available\n",
        "    while total_collected < TARGET_PAPERS:\n",
        "\n",
        "        data = fetch_data_from_api(QUERY, current_offset)\n",
        "\n",
        "        if data is None or 'data' not in data:\n",
        "            print(\"API returned no data or an error. Stopping collection.\")\n",
        "            break\n",
        "\n",
        "        papers = data['data']\n",
        "\n",
        "        if not papers:\n",
        "            print(\"No more papers found. Stopping collection.\")\n",
        "            break\n",
        "\n",
        "        print(f\"   -> Successfully fetched {len(papers)} papers. Current total: {total_collected + len(papers)}\")\n",
        "\n",
        "        for paper in papers:\n",
        "            # Clean and flatten the data before appending\n",
        "            abstract_text = paper.get('abstract', 'N/A')\n",
        "\n",
        "            # Skip papers without abstracts to ensure data quality\n",
        "            if abstract_text == 'N/A' or not abstract_text:\n",
        "                continue\n",
        "\n",
        "            authors = \", \".join([a['name'] for a in paper.get('authors', [])])\n",
        "\n",
        "            all_abstracts.append({\n",
        "                'Paper ID': paper.get('paperId'),\n",
        "                'Title': paper.get('title'),\n",
        "                'Year': paper.get('year'),\n",
        "                'Authors': authors,\n",
        "                'Abstract': abstract_text\n",
        "            })\n",
        "\n",
        "        # Update counters for the next loop\n",
        "        total_collected = len(all_abstracts)\n",
        "        current_offset += PAGE_LIMIT\n",
        "\n",
        "        # Check if we should stop\n",
        "        if total_collected >= TARGET_PAPERS:\n",
        "            print(f\"Target of {TARGET_PAPERS} papers reached.\")\n",
        "            break\n",
        "\n",
        "        # Respect API limits\n",
        "        time.sleep(SLEEP_TIME_SECONDS)\n",
        "\n",
        "    print(f\"\\n--- Collection Complete ---\")\n",
        "    print(f\"Total valid abstracts collected: {total_collected}\")\n",
        "\n",
        "    if all_abstracts:\n",
        "        # Save to CSV using pandas\n",
        "        df = pd.DataFrame(all_abstracts)\n",
        "        df.to_csv(CSV_FILENAME, index=False, encoding='utf-8')\n",
        "        print(f\"Data saved successfully to {os.path.abspath(CSV_FILENAME)}\")\n",
        "    else:\n",
        "        print(\"No abstracts were collected. CSV file not created.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure pandas and requests are installed:\n",
        "    # pip install pandas requests\n",
        "\n",
        "    # Running the collector\n",
        "    collect_and_save_abstracts()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkB-TH9qa4xg",
        "outputId": "dbb5aa0f-eba4-405f-e51f-3559b2e9ebc0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-> Fetching papers starting at offset 0...\n",
            "   -> Successfully fetched 100 papers. Current total: 100\n",
            "-> Fetching papers starting at offset 100...\n",
            "   -> Successfully fetched 100 papers. Current total: 159\n",
            "-> Fetching papers starting at offset 200...\n",
            "Rate limit hit (429). Sleeping for 10 seconds.\n",
            "-> Fetching papers starting at offset 200...\n",
            "   -> Successfully fetched 100 papers. Current total: 224\n",
            "-> Fetching papers starting at offset 300...\n",
            "   -> Successfully fetched 100 papers. Current total: 296\n",
            "-> Fetching papers starting at offset 400...\n",
            "Rate limit hit (429). Sleeping for 10 seconds.\n",
            "-> Fetching papers starting at offset 400...\n",
            "   -> Successfully fetched 100 papers. Current total: 346\n",
            "-> Fetching papers starting at offset 500...\n",
            "   -> Successfully fetched 100 papers. Current total: 387\n",
            "-> Fetching papers starting at offset 600...\n",
            "   -> Successfully fetched 100 papers. Current total: 439\n",
            "-> Fetching papers starting at offset 700...\n",
            "Rate limit hit (429). Sleeping for 10 seconds.\n",
            "-> Fetching papers starting at offset 700...\n",
            "Rate limit hit (429). Sleeping for 10 seconds.\n",
            "-> Fetching papers starting at offset 700...\n",
            "   -> Successfully fetched 100 papers. Current total: 471\n",
            "-> Fetching papers starting at offset 800...\n",
            "   -> Successfully fetched 100 papers. Current total: 487\n",
            "-> Fetching papers starting at offset 900...\n",
            "Rate limit hit (429). Sleeping for 10 seconds.\n",
            "-> Fetching papers starting at offset 900...\n",
            "   -> Successfully fetched 100 papers. Current total: 529\n",
            "-> Fetching papers starting at offset 1000...\n",
            "Rate limit hit (429). Sleeping for 10 seconds.\n",
            "-> Fetching papers starting at offset 1000...\n",
            "Rate limit hit (429). Sleeping for 10 seconds.\n",
            "-> Fetching papers starting at offset 1000...\n",
            "HTTP Error occurred: 400 Client Error: Bad Request for url: https://api.semanticscholar.org/graph/v1/paper/search?query=machine+learning&offset=1000&limit=100&fields=paperId%2Ctitle%2Cabstract%2Cauthors%2Cyear\n",
            "API returned no data or an error. Stopping collection.\n",
            "\n",
            "--- Collection Complete ---\n",
            "Total valid abstracts collected: 451\n",
            "Data saved successfully to /content/semantic_scholar_abstracts.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (15 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5QX6bJjGWXY9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import os\n",
        "import io\n",
        "\n",
        "# --- Configuration ---\n",
        "INPUT_FILE = \"semantic_scholar_abstracts.csv\"\n",
        "OUTPUT_FILE = \"semantic_scholar_abstracts_step_analysis.csv\"\n",
        "TARGET_COLUMN = \"Abstract\"\n",
        "\n",
        "# --- NLTK Setup ---\n",
        "# Downloads required NLTK data resources (needed for tokenization, stopwords, and lemmatization)\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab', quiet=True)\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "try:\n",
        "    nltk.data.find('corpora/omw-1.4')\n",
        "except LookupError:\n",
        "    nltk.download('omw-1.4', quiet=True)\n",
        "\n",
        "\n",
        "# Initialize NLTK tools globally for efficiency\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "english_stopwords = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Data Cleaning Functions (Mapped to User's Request Numbering) ---\n",
        "\n",
        "# We will run these functions in the standard NLP order (4, 1, 2, 3, 6, 5)\n",
        "# to ensure correctness, but name them according to the user's requested numbering.\n",
        "\n",
        "def clean_step_1_remove_noise_punct(text):\n",
        "    \"\"\"(1) Remove noise, such as special characters and punctuations.\"\"\"\n",
        "    # This step is applied AFTER lowercasing for better regex handling.\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return \"\"\n",
        "    # Keep only letters and spaces, replacing punctuation/special chars with a space\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    # Collapse multiple spaces and strip\n",
        "    return re.sub(r'\\s+', ' ', text).strip()"
      ],
      "metadata": {
        "id": "CtN1QFKLiwRl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_step_2_remove_numbers(text):\n",
        "    \"\"\"(2) Remove numbers.\"\"\"\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return \"\"\n",
        "    # Replace all digits with a single space\n",
        "    text = re.sub(r'\\d+', ' ', text)\n",
        "    # Collapse multiple spaces and strip\n",
        "    return re.sub(r'\\s+', ' ', text).strip()"
      ],
      "metadata": {
        "id": "yS2JzgbZi-IA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_step_3_remove_stopwords(text):\n",
        "    \"\"\"(3) Remove stopwords by using the stopwords list.\"\"\"\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return \"\"\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Filter out stopwords and single characters (often remnants of noise)\n",
        "    tokens = [word for word in tokens if word not in english_stopwords and len(word) > 1]\n",
        "    # Rejoin the processed tokens\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "A-dp7iEvjD63"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_step_4_lowercase(text):\n",
        "    \"\"\"(4) Lowercase all texts.\"\"\"\n",
        "    # NOTE: This step is logically the first cleaning step performed on the raw data.\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return \"\"\n",
        "    return text.lower()"
      ],
      "metadata": {
        "id": "W6KwMGL1jLGf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_step_5_stemming(text):\n",
        "    \"\"\"(5) Stemming (Applied after Lemmatization).\"\"\"\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return \"\"\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Stem tokens to their root\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "    # Rejoin the processed tokens\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "66wNoTnYjP-W"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_step_6_lemmatization(text):\n",
        "    \"\"\"(6) Lemmatization (Applied before Stemming).\"\"\"\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return \"\"\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Lemmatize tokens to their base form\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    # Rejoin the processed tokens\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "7GE8KBNmjWjS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dummy_data(file_name):\n",
        "    \"\"\"Creates a dummy CSV file if the actual file is not found, for demonstration.\"\"\"\n",
        "    data = {\n",
        "        'paperId': ['id1', 'id2', 'id3', 'id4'],\n",
        "        'title': ['Machine Learning in 2024', 'A Novel Algorithm for 5G Networks', 'The Future of AI and Robotics', 'Data Analysis with Python 3.9'],\n",
        "        'Abstract': [\n",
        "            \"This paper explores various machine learning models (like SVM and Neural Networks) and compares their performance on large datasets, achieving 98.5% accuracy.\",\n",
        "            \"Our research proposes a novel algorithm (ALGO-5G-B) for optimizing bandwidth allocation in complex 5G network architectures. Results show a 20% improvement.\",\n",
        "            \"Understanding the societal impact of artificial intelligence and advanced robotics systems in the next decade. Ethical considerations are paramount.\",\n",
        "            \"A tutorial on performing basic data cleaning, analysis, and visualization using the Python 3.9 ecosystem. No complex mathematics are involved.\"\n",
        "        ],\n",
        "        'year': [2024, 2023, 2025, 2022]\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(file_name, index=False)\n",
        "    print(f\"'{file_name}' not found. Created dummy data for demonstration.\")"
      ],
      "metadata": {
        "id": "dlcK-oaejbiR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main function to run the text cleaning pipeline and create step-by-step columns.\"\"\"\n",
        "\n",
        "    # 1. Load Data\n",
        "    try:\n",
        "        df = pd.read_csv(INPUT_FILE)\n",
        "    except FileNotFoundError:\n",
        "        create_dummy_data(INPUT_FILE)\n",
        "        df = pd.read_csv(INPUT_FILE)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading file {INPUT_FILE}: {e}\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n--- Starting Step-by-Step Text Cleaning Pipeline ---\")\n",
        "    print(f\"Input file: {INPUT_FILE} ({len(df)} rows)\")\n",
        "\n",
        "    if TARGET_COLUMN not in df.columns:\n",
        "        print(f\"Error: Target column '{TARGET_COLUMN}' not found. Please check your CSV column names.\")\n",
        "        return\n",
        "\n",
        "    # 2. Apply Cleaning Functions Sequentially (Standard NLP Order)\n",
        "    # NOTE: The steps below are applied in the order required for correct NLP processing.\n",
        "\n",
        "    # Step A (User's #4): Lowercase (Applied to Raw Data)\n",
        "    df['step_A_lowercase'] = df[TARGET_COLUMN].apply(clean_step_4_lowercase)\n",
        "\n",
        "    # Step B (User's #1): Remove Punctuation/Noise (Applied to Lowercased text)\n",
        "    df['step_B_no_punct'] = df['step_A_lowercase'].apply(clean_step_1_remove_noise_punct)\n",
        "\n",
        "    # Step C (User's #2): Remove Numbers (Applied to Punctuation-Free text)\n",
        "    df['step_C_no_numbers'] = df['step_B_no_punct'].apply(clean_step_2_remove_numbers)\n",
        "\n",
        "    # Step D (User's #3): Remove Stopwords (Applied to Number-Free text)\n",
        "    df['step_D_no_stopwords'] = df['step_C_no_numbers'].apply(clean_step_3_remove_stopwords)\n",
        "\n",
        "    # Step E (User's #6): Lemmatization (Applied to Stopword-Free text)\n",
        "    df['step_E_lemmatized'] = df['step_D_no_stopwords'].apply(clean_step_6_lemmatization)\n",
        "\n",
        "    # Step F (User's #5): Stemming (Applied to Lemmatized text - Final Output)\n",
        "    df['step_F_stemmed'] = df['step_E_lemmatized'].apply(clean_step_5_stemming)\n",
        "\n",
        "\n",
        "    # 3. Save Cleaned Data\n",
        "    df['cleaned_abstract_final'] = df['step_F_stemmed']\n",
        "    df.to_csv(OUTPUT_FILE, index=False)\n",
        "\n",
        "    print(f\"\\n--- Cleaning Complete ---\")\n",
        "    print(f\"Total rows processed: {len(df)}\")\n",
        "    print(f\"Cleaned data saved with all 6 steps in: {OUTPUT_FILE}\")\n",
        "\n",
        "    # Show step-by-step output based on the user's requested numbering (1-6)\n",
        "    print(\"\\n--- Demonstration of Cleaning Steps (First Row Transformation) ---\")\n",
        "    print(f\"Original Abstract: \\n  {df[TARGET_COLUMN].iloc[0]}\\n\")\n",
        "    print(f\"(4) Lowercase (Step A):\\n  {df['step_A_lowercase'].iloc[0]}\")\n",
        "    print(f\"(1) No Punctuation (Step B):\\n  {df['step_B_no_punct'].iloc[0]}\")\n",
        "    print(f\"(2) No Numbers (Step C):\\n  {df['step_C_no_numbers'].iloc[0]}\")\n",
        "    print(f\"(3) No Stopwords (Step D):\\n  {df['step_D_no_stopwords'].iloc[0]}\")\n",
        "    print(f\"(6) Lemmatization (Step E):\\n  {df['step_E_lemmatized'].iloc[0]}\")\n",
        "    print(f\"(5) Stemming/Final (Step F):\\n  {df['step_F_stemmed'].iloc[0]}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXydm1yGjvZY",
        "outputId": "e41d5804-931e-42f3-ac10-33a967569db3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Step-by-Step Text Cleaning Pipeline ---\n",
            "Input file: semantic_scholar_abstracts.csv (451 rows)\n",
            "\n",
            "--- Cleaning Complete ---\n",
            "Total rows processed: 451\n",
            "Cleaned data saved with all 6 steps in: semantic_scholar_abstracts_step_analysis.csv\n",
            "\n",
            "--- Demonstration of Cleaning Steps (First Row Transformation) ---\n",
            "Original Abstract: \n",
            "  We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at this https URL\n",
            "\n",
            "(4) Lowercase (Step A):\n",
            "  we present fashion-mnist, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. the training set has 60,000 images and the test set has 10,000 images. fashion-mnist is intended to serve as a direct drop-in replacement for the original mnist dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. the dataset is freely available at this https url\n",
            "(1) No Punctuation (Step B):\n",
            "  we present fashion mnist a new dataset comprising of 28x28 grayscale images of 70 000 fashion products from 10 categories with 7 000 images per category the training set has 60 000 images and the test set has 10 000 images fashion mnist is intended to serve as a direct drop in replacement for the original mnist dataset for benchmarking machine learning algorithms as it shares the same image size data format and the structure of training and testing splits the dataset is freely available at this https url\n",
            "(2) No Numbers (Step C):\n",
            "  we present fashion mnist a new dataset comprising of x grayscale images of fashion products from categories with images per category the training set has images and the test set has images fashion mnist is intended to serve as a direct drop in replacement for the original mnist dataset for benchmarking machine learning algorithms as it shares the same image size data format and the structure of training and testing splits the dataset is freely available at this https url\n",
            "(3) No Stopwords (Step D):\n",
            "  present fashion mnist new dataset comprising grayscale images fashion products categories images per category training set images test set images fashion mnist intended serve direct drop replacement original mnist dataset benchmarking machine learning algorithms shares image size data format structure training testing splits dataset freely available https url\n",
            "(6) Lemmatization (Step E):\n",
            "  present fashion mnist new dataset comprising grayscale image fashion product category image per category training set image test set image fashion mnist intended serve direct drop replacement original mnist dataset benchmarking machine learning algorithm share image size data format structure training testing split dataset freely available http url\n",
            "(5) Stemming/Final (Step F):\n",
            "  present fashion mnist new dataset compris grayscal imag fashion product categori imag per categori train set imag test set imag fashion mnist intend serv direct drop replac origin mnist dataset benchmark machin learn algorithm share imag size data format structur train test split dataset freeli avail http url\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (15 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19f5bc90-0103-4c51-a9e9-3e25023085e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy model loaded successfully.\n",
            "\n",
            "--- Starting Syntax and Structure Analysis Pipeline ---\n",
            "Input file: semantic_scholar_abstracts_step_analysis.csv (451 rows)\n",
            "All abstracts converted to spaCy Doc objects.\n",
            "-> (1) POS Tagging Complete.\n",
            "-> (2) Dependency Parsing Complete.\n",
            "-> (3) Named Entity Recognition Complete.\n",
            "\n",
            "--- Analysis Complete ---\n",
            "Results saved to: semantic_scholar_abstracts_syntax_analysis.csv\n",
            "\n",
            "--- Summary of Analysis for First Row ---\n",
            "Original Cleaned Text: present fashion mnist new dataset compris grayscal imag fashion product categori imag per categori train set imag test set imag fashion mnist intend serv direct drop replac origin mnist dataset benchmark machin learn algorithm share imag size data format structur train test split dataset freeli avail http url\n",
            "\n",
            "(1) POS Summary: N: 20, V: 7, Adj: 5, Adv: 0\n",
            "\n",
            "(2) Dependency Parse (Sample Sentence):\n",
            "present (amod) --> mnist | fashion (compound) --> mnist | mnist (nsubj) --> imag | new (amod) --> imag | dataset (compound) --> grayscal | compris (compound) --> grayscal | grayscal (compound) --> imag | imag (compound) --> product | fashion (compound) --> product | product (nsubj) --> set | categori (amod) --> product | imag (appos) --> categori | per (prep) --> imag | categori (nmod) --> train | train (pobj) --> per | set (ROOT) --> set | imag (compound) --> test | test (dobj) --> set | set (xcomp) --> set | imag (compound) --> mnist | fashion (compound) --> mnist | mnist (dobj) --> set | intend (conj) --> set | serv (amod) --> machin | direct (amod) --> drop | drop (compound) --> replac | replac (compound) --> origin | origin (compound) --> machin | mnist (compound) --> machin | dataset (compound) --> machin | benchmark (compound) --> machin | machin (nsubj) --> learn | learn (conj) --> set | algorithm (compound) --> share | share (dobj) --> learn | imag (compound) --> format | size (compound) --> format | data (compound) --> format | format (compound) --> test | structur (compound) --> test | train (compound) --> test | test (nsubj) --> split | split (conj) --> set | dataset (amod) --> url | freeli (amod) --> url | avail (compound) --> http | http (compound) --> url | url (dobj) --> split\n",
            "\n",
            "(3) NER Summary: Person: 1, Org: 0, Loc/GPE: 0, Product: 0, Date: 0\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from collections import Counter\n",
        "import os\n",
        "import io\n",
        "\n",
        "# NOTE ON LIBRARIES:\n",
        "# This script uses the powerful spaCy library for advanced analysis (NER and Dependency Parsing).\n",
        "# Before running, you must install spaCy and download its small English model:\n",
        "# pip install spacy\n",
        "# python -m spacy download en_core_web_sm\n",
        "\n",
        "# --- Configuration ---\n",
        "INPUT_FILE = \"semantic_scholar_abstracts_step_analysis.csv\"\n",
        "OUTPUT_FILE = \"semantic_scholar_abstracts_syntax_analysis.csv\"\n",
        "CLEANED_COLUMN = \"cleaned_abstract_final\" # The final stemmed text column from the previous script\n",
        "\n",
        "# Global spaCy model object\n",
        "nlp = None\n",
        "\n",
        "def load_spacy_model():\n",
        "    \"\"\"Loads the spaCy model, handling potential errors if not installed.\"\"\"\n",
        "    global nlp\n",
        "    try:\n",
        "        # Load the small English model\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "        print(\"spaCy model loaded successfully.\")\n",
        "    except OSError:\n",
        "        print(\"\\nERROR: spaCy model 'en_core_web_sm' not found.\")\n",
        "        print(\"Please run the following commands in your terminal:\")\n",
        "        print(\"  pip install spacy\")\n",
        "        print(\"  python -m spacy download en_core_web_sm\")\n",
        "        nlp = None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred while loading spaCy: {e}\")\n",
        "        nlp = None\n",
        "\n",
        "def create_dummy_data(file_name):\n",
        "    \"\"\"Creates a dummy CSV with cleaned text if the actual file is not found.\"\"\"\n",
        "    print(f\"'{file_name}' not found. Creating dummy data with a '{CLEANED_COLUMN}' column.\")\n",
        "    data = {\n",
        "        'paperId': ['id1', 'id2', 'id3', 'id4'],\n",
        "        CLEANED_COLUMN: [\n",
        "            \"paper explor variou machin learn model like svm neural network compar perform larg dataset achiev accuraci\",\n",
        "            \"research propos novel algorithm algo b optim bandwidth alloc complex network architectur result show improv\",\n",
        "            \"understand societal impact artificial intellig advanc robot system next decad ethic consider paramount\",\n",
        "            \"tutori perform basic data clean analysi visual python ecosystem complex mathemat involv\"\n",
        "        ]\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    # The dummy data is intentionally stemmed and lowercase, mimicking the input.\n",
        "    df.to_csv(file_name, index=False)\n",
        "    print(\"Dummy file created. Proceeding with analysis...\")\n",
        "    return df\n",
        "\n",
        "# --- (1) Parts of Speech (POS) Tagging Analysis ---\n",
        "\n",
        "def analyze_pos(doc):\n",
        "    \"\"\"\n",
        "    Tags Parts of Speech and calculates the count of Noun, Verb, Adj, and Adv.\n",
        "    Uses spaCy's coarse-grained tags (e.g., NOUN, VERB).\n",
        "    \"\"\"\n",
        "    pos_counts = Counter()\n",
        "    pos_tags = []\n",
        "\n",
        "    for token in doc:\n",
        "        # Get the coarse-grained Universal POS tag (NOUN, VERB, ADJ, ADV)\n",
        "        pos_tag = token.pos_\n",
        "        pos_tags.append(f\"{token.text}/{pos_tag}\")\n",
        "\n",
        "        if pos_tag in ['NOUN', 'VERB', 'ADJ', 'ADV']:\n",
        "            # Count the required POS categories\n",
        "            pos_counts[pos_tag] += 1\n",
        "\n",
        "    # Format the results for the DataFrame column\n",
        "    summary = (f\"N: {pos_counts['NOUN']}, \"\n",
        "               f\"V: {pos_counts['VERB']}, \"\n",
        "               f\"Adj: {pos_counts['ADJ']}, \"\n",
        "               f\"Adv: {pos_counts['ADV']}\")\n",
        "\n",
        "    return ' '.join(pos_tags), summary\n",
        "\n",
        "# --- (2) Dependency Parsing Analysis ---\n",
        "# NOTE: Constituency Parsing structure is explained in the 'analysis_explanation.md' file.\n",
        "\n",
        "def analyze_dependency_parsing(doc):\n",
        "    \"\"\"\n",
        "    Analyzes dependency parsing for all sentences in a text.\n",
        "    Returns a string representation of the dependency tree for each sentence.\n",
        "    \"\"\"\n",
        "    dependency_trees = []\n",
        "\n",
        "    # Iterate over sentences segmented by spaCy\n",
        "    for sent in doc.sents:\n",
        "        tree_lines = []\n",
        "        # Iterate over tokens in the sentence to build the dependency structure\n",
        "        for token in sent:\n",
        "            # Format: TOKEN (DEP_RELATION) --> HEAD_TOKEN\n",
        "            line = (\n",
        "                f\"{token.text} ({token.dep_}) --> {token.head.text}\"\n",
        "            )\n",
        "            tree_lines.append(line)\n",
        "\n",
        "        # Add the visualization for the whole sentence\n",
        "        dependency_trees.append(\" | \".join(tree_lines))\n",
        "\n",
        "    return \"\\n\".join(dependency_trees)\n",
        "\n",
        "# --- (3) Named Entity Recognition (NER) Analysis ---\n",
        "\n",
        "def analyze_ner(doc):\n",
        "    \"\"\"\n",
        "    Extracts all named entities and calculates the count of each entity type specified.\n",
        "    \"\"\"\n",
        "    entity_counts = Counter()\n",
        "    entities_list = []\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        # ent.text is the entity text, ent.label_ is the type (e.g., ORG, PERSON)\n",
        "        entities_list.append(f\"{ent.text}/{ent.label_}\")\n",
        "\n",
        "        # Count only the types relevant to the user's request\n",
        "        if ent.label_ in ['PERSON', 'ORG', 'GPE', 'LOC', 'PRODUCT', 'DATE']:\n",
        "            entity_counts[ent.label_] += 1\n",
        "\n",
        "    # Format the results for the DataFrame column\n",
        "    summary = (f\"Person: {entity_counts['PERSON']}, \"\n",
        "               f\"Org: {entity_counts['ORG']}, \"\n",
        "               f\"Loc/GPE: {entity_counts['LOC'] + entity_counts['GPE']}, \"\n",
        "               f\"Product: {entity_counts['PRODUCT']}, \"\n",
        "               f\"Date: {entity_counts['DATE']}\")\n",
        "\n",
        "    return ' '.join(entities_list), summary\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the syntax and structure analysis pipeline.\"\"\"\n",
        "\n",
        "    # 0. Initialize spaCy\n",
        "    load_spacy_model()\n",
        "    if nlp is None:\n",
        "        return # Exit if model failed to load\n",
        "\n",
        "    # 1. Load Data\n",
        "    try:\n",
        "        df = pd.read_csv(INPUT_FILE)\n",
        "    except FileNotFoundError:\n",
        "        df = create_dummy_data(INPUT_FILE)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading file {INPUT_FILE}: {e}\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n--- Starting Syntax and Structure Analysis Pipeline ---\")\n",
        "    print(f\"Input file: {INPUT_FILE} ({len(df)} rows)\")\n",
        "\n",
        "    if CLEANED_COLUMN not in df.columns:\n",
        "        print(f\"Error: Required cleaned column '{CLEANED_COLUMN}' not found in the input CSV.\")\n",
        "        print(\"Please ensure you ran the previous cleaning script successfully.\")\n",
        "        return\n",
        "\n",
        "    # 2. Process Text and Apply Analyses\n",
        "\n",
        "    # Create a spaCy Doc object for each row for efficient processing\n",
        "    # The text is already stemmed, which may impact POS and NER accuracy, but we proceed with the cleaned text as instructed.\n",
        "    df['spacy_doc'] = df[CLEANED_COLUMN].apply(lambda text: nlp(str(text)) if pd.notna(text) else nlp(\"\"))\n",
        "    print(\"All abstracts converted to spaCy Doc objects.\")\n",
        "\n",
        "    # (1) Parts of Speech (POS) Tagging\n",
        "    # The zip(*...) unpacks the tuple return (tags_string, summary_string) into two separate columns\n",
        "    df['pos_tags'], df['pos_summary'] = zip(*df['spacy_doc'].apply(analyze_pos))\n",
        "    print(\"-> (1) POS Tagging Complete.\")\n",
        "\n",
        "    # (2) Dependency Parsing\n",
        "    df['dependency_tree'] = df['spacy_doc'].apply(analyze_dependency_parsing)\n",
        "    print(\"-> (2) Dependency Parsing Complete.\")\n",
        "\n",
        "    # (3) Named Entity Recognition (NER)\n",
        "    df['ner_entities'], df['ner_summary'] = zip(*df['spacy_doc'].apply(analyze_ner))\n",
        "    print(\"-> (3) Named Entity Recognition Complete.\")\n",
        "\n",
        "\n",
        "    # 3. Save Results\n",
        "    # Select original columns plus new analysis columns for the output file\n",
        "\n",
        "    # Create the list of new columns\n",
        "    analysis_columns = ['pos_summary', 'ner_summary', 'pos_tags', 'dependency_tree', 'ner_entities']\n",
        "\n",
        "    # Identify existing columns and remove the temporary 'spacy_doc'\n",
        "    output_columns = df.columns.tolist()\n",
        "    if 'spacy_doc' in output_columns:\n",
        "        output_columns.remove('spacy_doc')\n",
        "\n",
        "    # Reorder columns to put original data first, then the analysis summaries/details\n",
        "    final_output_columns = [col for col in output_columns if col not in analysis_columns] + analysis_columns\n",
        "\n",
        "    df[final_output_columns].to_csv(OUTPUT_FILE, index=False)\n",
        "\n",
        "    print(f\"\\n--- Analysis Complete ---\")\n",
        "    print(f\"Results saved to: {OUTPUT_FILE}\")\n",
        "\n",
        "    # Display the analysis for the first row\n",
        "    print(\"\\n--- Summary of Analysis for First Row ---\")\n",
        "    first_row = df.iloc[0]\n",
        "    print(f\"Original Cleaned Text: {first_row[CLEANED_COLUMN]}\")\n",
        "    print(f\"\\n(1) POS Summary: {first_row['pos_summary']}\")\n",
        "    print(f\"\\n(2) Dependency Parse (Sample Sentence):\\n{first_row['dependency_tree'].split('\\n')[0]}\")\n",
        "    print(f\"\\n(3) NER Summary: {first_row['ner_summary']}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Following Questions must answer using AI assitance**"
      ],
      "metadata": {
        "id": "EcVqy1yj3wja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 4 (20 points)."
      ],
      "metadata": {
        "id": "kEdcyHX8VaDB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. (PART-1)\n",
        "Web scraping data from the GitHub Marketplace to gather details about popular actions. Using Python, the process begins by sending HTTP requests to multiple pages of the marketplace (1000 products), handling pagination through dynamic page numbers. The key details extracted include the product name, a short description, and the URL.\n",
        "\n",
        " The extracted data is stored in a structured CSV format with columns for product name, description, URL, and page number. A time delay is introduced between requests to avoid server overload. ChatGPT can assist by helping with the parsing of HTML, error handling, and generating reports based on the data collected.\n",
        "\n",
        " The goal is to complete the scraping within a specified time limit, ensuring that the process is efficient and adheres to GitHub’s usage guidelines.\n",
        "\n",
        "(PART -2)\n",
        "\n",
        "1.   **Preprocess Data**: Clean the text by tokenizing, removing stopwords, and converting to lowercase.\n",
        "\n",
        "2. Perform **Data Quality** operations.\n",
        "\n",
        "\n",
        "Preprocessing:\n",
        "Preprocessing involves cleaning the text by removing noise such as special characters, HTML tags, and unnecessary whitespace. It also includes tasks like tokenization, stopword removal, and lemmatization to standardize the text for analysis.\n",
        "\n",
        "Data Quality:\n",
        "Data quality checks ensure completeness, consistency, and accuracy by verifying that all required columns are filled and formatted correctly. Additionally, it involves identifying and removing duplicates, handling missing values, and ensuring the data reflects the true content accurately.\n"
      ],
      "metadata": {
        "id": "1Ung5_YW3C6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Github MarketPlace page:\n",
        "https://github.com/marketplace?type=actions"
      ],
      "metadata": {
        "id": "CTOfUpatronW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 1\n",
        "import random, requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "BASE_URL = \"https://github.com/marketplace?type=actions\"\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (compatible; GH-ResearchBot/1.0)\",\n",
        "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "}\n",
        "\n",
        "def get_page_html(page: int, retries=3, timeout=20):\n",
        "    url = f\"{BASE_URL}&page={page}\"\n",
        "    last_err = None\n",
        "    for attempt in range(1, retries+1):\n",
        "        try:\n",
        "            r = requests.get(url, headers=HEADERS, timeout=timeout)\n",
        "            # If GitHub ever rate-limits, back off and retry\n",
        "            if r.status_code == 429:\n",
        "                time.sleep(attempt * 2.0)\n",
        "                continue\n",
        "            r.raise_for_status()\n",
        "            html = r.text\n",
        "            # Quick human-check/captcha guard\n",
        "            if \"please verify you are a human\" in html.lower():\n",
        "                raise RuntimeError(\"Blocked by anti-bot / CAPTCHA\")\n",
        "            return html\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            time.sleep(attempt * 1.5)\n",
        "    raise RuntimeError(f\"Failed page {page}: {last_err}\")\n",
        "\n",
        "def parse_actions(html: str, page: int):\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    out = []\n",
        "    # Anchor on the action detail link; classes change, href pattern is stable\n",
        "    for a in soup.select('a[href^=\"/marketplace/actions/\"]'):\n",
        "        href = a.get(\"href\", \"\")\n",
        "        # Avoid top nav/side links by focusing on list area:\n",
        "        # walk up to a reasonable container then mine fields\n",
        "        container = a.find_parent([\"article\", \"li\", \"div\", \"section\"]) or a.parent\n",
        "\n",
        "        # Name: prefer the anchor text or the nearest <h3>\n",
        "        name = a.get_text(\" \", strip=True)\n",
        "        if not name:\n",
        "            h3 = container.find(\"h3\")\n",
        "            name = h3.get_text(\" \", strip=True) if h3 else \"\"\n",
        "\n",
        "        # Description: common pattern is a muted <p> or first <p> in container\n",
        "        desc = \"\"\n",
        "        p = container.find(\"p\")\n",
        "        if p:\n",
        "            desc = p.get_text(\" \", strip=True)\n",
        "\n",
        "        # Normalize URL\n",
        "        url = f\"https://github.com{href}\" if href.startswith(\"/\") else href\n",
        "\n",
        "        if name and url:\n",
        "            out.append({\"Product Name\": name, \"Description\": desc, \"URL\": url, \"Page Number\": page})\n",
        "    return out\n",
        "\n",
        "# ---- scrape loop (keeps your config) ----\n",
        "TARGET_COUNT = 1000      # GitHub typically caps listings to ~1000 results across pagination. :contentReference[oaicite:1]{index=1}\n",
        "MAX_PAGES = 500          # safety cap (page links show up to 500 currently). :contentReference[oaicite:2]{index=2}\n",
        "DELAY_RANGE = (1.0, 2.0) # polite delay\n",
        "\n",
        "records = []\n",
        "page = 1\n",
        "while len(records) < TARGET_COUNT and page <= MAX_PAGES:\n",
        "    html = get_page_html(page)\n",
        "    rows = parse_actions(html, page)\n",
        "\n",
        "    # de-dup across pages by URL before extending\n",
        "    before = len(records)\n",
        "    seen = {r[\"URL\"] for r in records}\n",
        "    rows = [r for r in rows if r[\"URL\"] not in seen]\n",
        "    records.extend(rows)\n",
        "\n",
        "    print(f\"Page {page}: found {len(rows)} actions (total so far: {len(records)})\")\n",
        "\n",
        "    # Stop if the page obviously has no results (end of pagination)\n",
        "    if len(rows) == 0:\n",
        "        break\n",
        "\n",
        "    time.sleep(random.uniform(*DELAY_RANGE))\n",
        "    page += 1\n",
        "\n",
        "# Save & continue with your Part-2 cleaning\n",
        "raw_df = pd.DataFrame(records, columns=['Product Name','Description','URL','Page Number'])\n",
        "raw_df.to_csv('github_actions_raw.csv', index=False)\n",
        "print(f\"✅ Scraped {len(raw_df)} actions across {raw_df['Page Number'].nunique()} pages.\")\n",
        "print(raw_df.head())\n"
      ],
      "metadata": {
        "id": "4dtco9K--ks6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "525a1fd9-51f5-401d-ee4a-005eaa17e899"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page 1: found 20 actions (total so far: 20)\n",
            "Page 2: found 20 actions (total so far: 40)\n",
            "Page 3: found 20 actions (total so far: 60)\n",
            "Page 4: found 0 actions (total so far: 60)\n",
            "✅ Scraped 60 actions across 3 pages.\n",
            "                   Product Name Description  \\\n",
            "0                TruffleHog OSS               \n",
            "1                 Metrics embed               \n",
            "2  yq - portable yaml processor               \n",
            "3                  Super-Linter               \n",
            "4        Gosec Security Checker               \n",
            "\n",
            "                                                 URL  Page Number  \n",
            "0  https://github.com/marketplace/actions/truffle...            1  \n",
            "1  https://github.com/marketplace/actions/metrics...            1  \n",
            "2  https://github.com/marketplace/actions/yq-port...            1  \n",
            "3  https://github.com/marketplace/actions/super-l...            1  \n",
            "4  https://github.com/marketplace/actions/gosec-s...            1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === PART-2: Preprocess Data + Data Quality for GitHub Marketplace Actions ===\n",
        "import re, os, pandas as pd, nltk\n",
        "from collections import Counter\n",
        "\n",
        "# --- Ensure NLTK resources ---\n",
        "try: nltk.data.find(\"corpora/stopwords\")\n",
        "except LookupError: nltk.download(\"stopwords\", quiet=True)\n",
        "try: nltk.data.find(\"corpora/wordnet\")\n",
        "except LookupError: nltk.download(\"wordnet\", quiet=True)\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "STOPWORDS = set(stopwords.words(\"english\"))\n",
        "LEMMATIZER = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Clean text by:\n",
        "      - removing HTML tags\n",
        "      - lowercasing\n",
        "      - removing special chars\n",
        "      - tokenizing on whitespace\n",
        "      - removing stopwords / 1-char tokens\n",
        "      - lemmatizing\n",
        "    Returns a single space-joined normalized string.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    txt = re.sub(r\"<[^>]+>\", \" \", text)            # strip HTML\n",
        "    txt = re.sub(r\"\\s+\", \" \", txt).strip().lower() # normalize whitespace + lowercase\n",
        "    txt = re.sub(r\"[^a-z0-9 ]+\", \" \", txt)         # keep alnum and space\n",
        "    toks = [t for t in txt.split() if len(t) > 1 and t not in STOPWORDS]\n",
        "    toks = [LEMMATIZER.lemmatize(t) for t in toks]\n",
        "    return \" \".join(toks)\n",
        "\n",
        "# --- Load raw data (prefer in-memory raw_df; fallback to CSV) ---\n",
        "if \"raw_df\" in globals() and isinstance(raw_df, pd.DataFrame):\n",
        "    df = raw_df.copy()\n",
        "else:\n",
        "    src = \"github_actions_raw.csv\"\n",
        "    if not os.path.exists(src):\n",
        "        raise FileNotFoundError(\"github_actions_raw.csv not found. Run PART-1 first.\")\n",
        "    df = pd.read_csv(src)\n",
        "\n",
        "# --- DATA QUALITY OPERATIONS ---\n",
        "# 1) Required columns present\n",
        "required_cols = [\"Product Name\", \"Description\", \"URL\", \"Page Number\"]\n",
        "missing_cols = [c for c in required_cols if c not in df.columns]\n",
        "if missing_cols:\n",
        "    raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
        "\n",
        "initial_rows = len(df)\n",
        "\n",
        "# 2) Drop rows missing essential keys\n",
        "df.dropna(subset=[\"Product Name\", \"URL\"], inplace=True)\n",
        "\n",
        "# 3) Normalize URL + filter to github-only\n",
        "df[\"URL\"] = df[\"URL\"].astype(str).str.strip()\n",
        "df = df[df[\"URL\"].str.startswith(\"https://github.com/\")]\n",
        "\n",
        "# 4) Dedupe primarily by URL\n",
        "before_dedup = len(df)\n",
        "df.drop_duplicates(subset=[\"URL\"], inplace=True)\n",
        "duplicates_removed = before_dedup - len(df)\n",
        "\n",
        "# 5) Coerce types\n",
        "df[\"Page Number\"] = pd.to_numeric(df[\"Page Number\"], errors=\"coerce\").fillna(-1).astype(int)\n",
        "\n",
        "# 6) Handle missing description (allowed but normalized)\n",
        "df[\"Description\"] = df[\"Description\"].fillna(\"\").astype(str)\n",
        "\n",
        "# --- PREPROCESSING (tokenize/lower/stopwords/lemmatize) ---\n",
        "df[\"Product Name Clean\"] = df[\"Product Name\"].apply(preprocess_text)\n",
        "df[\"Description Clean\"] = df[\"Description\"].apply(preprocess_text)\n",
        "\n",
        "# 7) Keep rows that still have a meaningful name after cleaning\n",
        "before_name_filter = len(df)\n",
        "df = df[df[\"Product Name Clean\"].str.len() > 0]\n",
        "name_filtered_removed = before_name_filter - len(df)\n",
        "\n",
        "# --- SMALL REPORTS / ARTIFACTS ---\n",
        "# Token frequency (top 50) across both fields\n",
        "all_tokens = (df[\"Product Name Clean\"].fillna(\"\").str.cat(df[\"Description Clean\"].fillna(\"\"), sep=\" \").str.split())\n",
        "flat_tokens = [t for sub in all_tokens for t in sub]\n",
        "freq = Counter(flat_tokens).most_common(50)\n",
        "freq_df = pd.DataFrame(freq, columns=[\"token\", \"count\"])\n",
        "freq_df.to_csv(\"token_frequency_top50.csv\", index=False)\n",
        "\n",
        "# Final save\n",
        "clean_path = \"github_actions_cleaned.csv\"\n",
        "df.to_csv(clean_path, index=False)\n",
        "\n",
        "# Data Quality summary\n",
        "report = {\n",
        "    \"initial_rows\": int(initial_rows),\n",
        "    \"after_required_keys\": int(len(df) + duplicates_removed + name_filtered_removed),\n",
        "    \"duplicates_removed\": int(duplicates_removed),\n",
        "    \"name_filtered_rows_removed\": int(name_filtered_removed),\n",
        "    \"final_rows\": int(len(df)),\n",
        "    \"unique_urls\": int(df[\"URL\"].nunique()),\n",
        "    \"rows_with_empty_description\": int((df[\"Description\"].str.strip()==\"\").sum()),\n",
        "    \"unique_pages\": int(df[\"Page Number\"].nunique()),\n",
        "}\n",
        "print(\"📊 Data Quality Report\")\n",
        "for k,v in report.items():\n",
        "    print(f\"- {k}: {v}\")\n",
        "\n",
        "print(f\"\\n✅ Saved cleaned dataset → {clean_path}\")\n",
        "print(\"🧾 Saved token frequency (top 50) → token_frequency_top50.csv\")\n",
        "\n",
        "# Quick preview\n",
        "df.head(10)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "id": "hkJKfknIoHPw",
        "outputId": "0516e4af-c27d-441e-b5c8-7874db950241"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Data Quality Report\n",
            "- initial_rows: 60\n",
            "- after_required_keys: 60\n",
            "- duplicates_removed: 0\n",
            "- name_filtered_rows_removed: 0\n",
            "- final_rows: 60\n",
            "- unique_urls: 60\n",
            "- rows_with_empty_description: 60\n",
            "- unique_pages: 3\n",
            "\n",
            "✅ Saved cleaned dataset → github_actions_cleaned.csv\n",
            "🧾 Saved token frequency (top 50) → token_frequency_top50.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                        Product Name Description  \\\n",
              "0                                     TruffleHog OSS               \n",
              "1                                      Metrics embed               \n",
              "2                       yq - portable yaml processor               \n",
              "3                                       Super-Linter               \n",
              "4                             Gosec Security Checker               \n",
              "5                         Rebuild Armbian and Kernel               \n",
              "6                                           Checkout               \n",
              "7             OpenCommit — improve commits with AI 🧙               \n",
              "8                                SSH Remote Commands               \n",
              "9  generate-snake-game-from-github-contribution-grid               \n",
              "\n",
              "                                                 URL  Page Number  \\\n",
              "0  https://github.com/marketplace/actions/truffle...            1   \n",
              "1  https://github.com/marketplace/actions/metrics...            1   \n",
              "2  https://github.com/marketplace/actions/yq-port...            1   \n",
              "3  https://github.com/marketplace/actions/super-l...            1   \n",
              "4  https://github.com/marketplace/actions/gosec-s...            1   \n",
              "5  https://github.com/marketplace/actions/rebuild...            1   \n",
              "6    https://github.com/marketplace/actions/checkout            1   \n",
              "7  https://github.com/marketplace/actions/opencom...            1   \n",
              "8  https://github.com/marketplace/actions/ssh-rem...            1   \n",
              "9  https://github.com/marketplace/actions/generat...            1   \n",
              "\n",
              "                             Product Name Clean Description Clean  \n",
              "0                                 trufflehog os                    \n",
              "1                                  metric embed                    \n",
              "2                    yq portable yaml processor                    \n",
              "3                                  super linter                    \n",
              "4                        gosec security checker                    \n",
              "5                        rebuild armbian kernel                    \n",
              "6                                      checkout                    \n",
              "7                 opencommit improve commits ai                    \n",
              "8                            ssh remote command                    \n",
              "9  generate snake game github contribution grid                    "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fe7b89fc-14bd-4e6e-8875-11e4da696ca6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Product Name</th>\n",
              "      <th>Description</th>\n",
              "      <th>URL</th>\n",
              "      <th>Page Number</th>\n",
              "      <th>Product Name Clean</th>\n",
              "      <th>Description Clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TruffleHog OSS</td>\n",
              "      <td></td>\n",
              "      <td>https://github.com/marketplace/actions/truffle...</td>\n",
              "      <td>1</td>\n",
              "      <td>trufflehog os</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Metrics embed</td>\n",
              "      <td></td>\n",
              "      <td>https://github.com/marketplace/actions/metrics...</td>\n",
              "      <td>1</td>\n",
              "      <td>metric embed</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>yq - portable yaml processor</td>\n",
              "      <td></td>\n",
              "      <td>https://github.com/marketplace/actions/yq-port...</td>\n",
              "      <td>1</td>\n",
              "      <td>yq portable yaml processor</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Super-Linter</td>\n",
              "      <td></td>\n",
              "      <td>https://github.com/marketplace/actions/super-l...</td>\n",
              "      <td>1</td>\n",
              "      <td>super linter</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Gosec Security Checker</td>\n",
              "      <td></td>\n",
              "      <td>https://github.com/marketplace/actions/gosec-s...</td>\n",
              "      <td>1</td>\n",
              "      <td>gosec security checker</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Rebuild Armbian and Kernel</td>\n",
              "      <td></td>\n",
              "      <td>https://github.com/marketplace/actions/rebuild...</td>\n",
              "      <td>1</td>\n",
              "      <td>rebuild armbian kernel</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Checkout</td>\n",
              "      <td></td>\n",
              "      <td>https://github.com/marketplace/actions/checkout</td>\n",
              "      <td>1</td>\n",
              "      <td>checkout</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>OpenCommit — improve commits with AI 🧙</td>\n",
              "      <td></td>\n",
              "      <td>https://github.com/marketplace/actions/opencom...</td>\n",
              "      <td>1</td>\n",
              "      <td>opencommit improve commits ai</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>SSH Remote Commands</td>\n",
              "      <td></td>\n",
              "      <td>https://github.com/marketplace/actions/ssh-rem...</td>\n",
              "      <td>1</td>\n",
              "      <td>ssh remote command</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>generate-snake-game-from-github-contribution-grid</td>\n",
              "      <td></td>\n",
              "      <td>https://github.com/marketplace/actions/generat...</td>\n",
              "      <td>1</td>\n",
              "      <td>generate snake game github contribution grid</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fe7b89fc-14bd-4e6e-8875-11e4da696ca6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fe7b89fc-14bd-4e6e-8875-11e4da696ca6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fe7b89fc-14bd-4e6e-8875-11e4da696ca6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-d03b9916-1f64-456d-b957-b59fd3daf6d3\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d03b9916-1f64-456d-b957-b59fd3daf6d3')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-d03b9916-1f64-456d-b957-b59fd3daf6d3 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 60,\n  \"fields\": [\n    {\n      \"column\": \"Product Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 60,\n        \"samples\": [\n          \"TruffleHog OSS\",\n          \"Rebuild Armbian and Kernel\",\n          \"Paths Changes Filter\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Description\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"URL\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 60,\n        \"samples\": [\n          \"https://github.com/marketplace/actions/trufflehog-oss\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Page Number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Product Name Clean\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 60,\n        \"samples\": [\n          \"trufflehog os\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Description Clean\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 5 (20 points)\n",
        "\n",
        "PART 1:\n",
        "Web Scrape  tweets from Twitter using the Tweepy API, specifically targeting hashtags related to subtopics (machine learning or artificial intelligence.)\n",
        "The extracted data includes the tweet ID, username, and text.\n",
        "\n",
        "Part 2:\n",
        "Perform data cleaning procedures\n",
        "\n",
        "A final data quality check ensures the completeness and consistency of the dataset. The cleaned data is then saved into a CSV file for further analysis.\n",
        "\n",
        "\n",
        "**Note**\n",
        "\n",
        "1.   Follow tutorials provided in canvas to obtain api keys. Use ChatGPT to get the code. Make sure the file is downloaded and saved.\n",
        "2.   Make sure you divide GPT code as shown in tutorials, dont make multiple requestes.\n"
      ],
      "metadata": {
        "id": "3WeD70ty3Gui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Tweepy\n",
        "!pip install tweepy\n",
        "\n",
        "import tweepy\n",
        "import pandas as pd\n",
        "\n",
        "# Replace with your actual Bearer Token from Twitter Developer Portal\n",
        "bearer_token = \"AAAAAAAAAAAAAAAAAAAAAK734QEAAAAAXTwiK1bd4xYcPYRDfzIk%2FrKepx8%3DKbGbuWcJvxDe8hie2lJP39zTmw1HVHZmwBRMbnvnmiPYnAXcO1\"\n",
        "\n",
        "# Authenticate with Tweepy v2\n",
        "client = tweepy.Client(bearer_token=bearer_token)\n",
        "\n",
        "# Define query and parameters\n",
        "query = \"#machinelearning OR #artificialintelligence -is:retweet lang:en\"\n",
        "max_results = 100\n",
        "\n",
        "# Fetch tweets\n",
        "response = client.search_recent_tweets(\n",
        "    query=query,\n",
        "    max_results=max_results,\n",
        "    tweet_fields=[\"id\", \"text\", \"author_id\"]\n",
        ")\n",
        "\n",
        "# Extract data\n",
        "tweets_data = []\n",
        "if response.data:\n",
        "    for tweet in response.data:\n",
        "        tweets_data.append({\n",
        "            \"Tweet ID\": tweet.id,\n",
        "            \"Username\": tweet.author_id,\n",
        "            \"Text\": tweet.text\n",
        "        })\n",
        "\n",
        "# Save raw data\n",
        "df_raw = pd.DataFrame(tweets_data)\n",
        "df_raw.to_csv(\"raw_tweets.csv\", index=False)\n",
        "print(f\"✅ Scraped {len(df_raw)} tweets.\")\n",
        "print(df_raw.head())\n"
      ],
      "metadata": {
        "id": "qYRO5Cn8bYwZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b2e48e3-f6f4-426a-f241-caf72d3c2009"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.12/dist-packages (4.16.0)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from tweepy) (3.3.1)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.12/dist-packages (from tweepy) (2.32.4)\n",
            "Requirement already satisfied: requests-oauthlib<3,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from tweepy) (2.0.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27.0->tweepy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27.0->tweepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27.0->tweepy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27.0->tweepy) (2025.8.3)\n",
            "✅ Scraped 100 tweets.\n",
            "              Tweet ID            Username  \\\n",
            "0  1972810628223156645  945828015535038464   \n",
            "1  1972810623030554629           952502046   \n",
            "2  1972810594312192041          3068484222   \n",
            "3  1972810311154704578           178563985   \n",
            "4  1972810280725008808           568641861   \n",
            "\n",
            "                                                Text  \n",
            "0  ⏪🪐\\nIf you want to make money investing in sto...  \n",
            "1  For creative minds https://t.co/9ilgTaZESd\\nPl...  \n",
            "2  RT @rasangarocks: The best books to learn Pyth...  \n",
            "3  RT @jblefevre60: 5 project to learn AI!\\n\\n#AI...  \n",
            "4  RT @We_Promote_All: 🚀 Unlock the secrets of AI...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Load raw data\n",
        "df = pd.read_csv(\"raw_tweets.csv\")\n",
        "\n",
        "# Clean text\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r\"@\\w+\", \"\", text)\n",
        "    text = re.sub(r\"#\\w+\", \"\", text)\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "    return text.strip()\n",
        "\n",
        "df[\"Cleaned Text\"] = df[\"Text\"].apply(clean_text)\n",
        "\n",
        "# Drop duplicates and missing values\n",
        "df.drop_duplicates(subset=\"Tweet ID\", inplace=True)\n",
        "df.dropna(subset=[\"Tweet ID\", \"Username\", \"Cleaned Text\"], inplace=True)\n",
        "\n",
        "# Save cleaned data\n",
        "df.to_csv(\"cleaned_tweets.csv\", index=False)\n",
        "print(f\"\\n✅ Cleaned data saved. Final record count: {len(df)}\")\n",
        "print(df[[\"Tweet ID\", \"Username\", \"Cleaned Text\"]].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6csNB5P5Eb_",
        "outputId": "4173859e-df3b-44ad-dcdc-4abe44b2a6f3"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Cleaned data saved. Final record count: 100\n",
            "              Tweet ID            Username  \\\n",
            "0  1972810628223156645  945828015535038464   \n",
            "1  1972810623030554629           952502046   \n",
            "2  1972810594312192041          3068484222   \n",
            "3  1972810311154704578           178563985   \n",
            "4  1972810280725008808           568641861   \n",
            "\n",
            "                                        Cleaned Text  \n",
            "0  If you want to make money investing in stocks ...  \n",
            "1  For creative minds \\nPlay inside of your  with...  \n",
            "2  RT  The best books to learn Python programming...  \n",
            "3                          RT  5 project to learn AI  \n",
            "4  RT   Unlock the secrets of AI amp speech recog...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write your response below\n",
        "Fill out survey and provide your valuable feedback.\n",
        "\n",
        "https://docs.google.com/forms/d/e/1FAIpQLSd_ObuA3iNoL7Az_C-2NOfHodfKCfDzHZtGRfIker6WyZqTtA/viewform?usp=dialog"
      ],
      "metadata": {
        "id": "JbTa-jDS-KFI"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}